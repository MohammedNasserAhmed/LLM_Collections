{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPn5LAiSbttPjJ6WkPZrnnW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## ***Unleashing the Power of the Quantized Models from the Hugging Face Community: A Step-by-Step Guide*** ðŸ’»\n","\n","This notebook empowers you to learn how to utilize the Quantized Llama-2-13B model. The code demonstrates how to install necessary libraries, download the Quantized model, configure the Llama LLM for GPU acceleration, and interact with the model to solve a sample problem.\n","\n","### ðŸ”² ***Goal :***\n","\n","*This notebook aims to :*\n","\n","* Install required libraries for managing the Llama LLM and interacting with Hugging Face Hub.\n","\n","* Download the Quantized Llama-2-13B model specifically fine-tuned for chat.\n","\n","* Set up the Llama LLM for GPU processing to maximize performance.\n","Craft a prompt template incorporating user input and assistant role definition.\n","\n","* Generate a response using the model and extract the answer."],"metadata":{"id":"_M3vZGnroK6N"}},{"cell_type":"markdown","source":["### ðŸ”² ***Steps :***\n","\n","âœ… ***Install and Upgrade Libraries :***\n","\n","* The code starts by installing (or upgrading) necessary libraries:\n","\n"," * **llama-cpp-python**: Provides core functionalities to interact with the Llama LLM.\n","\n"," * **numpy**: Used for numerical computations (potentially involved in the model's processing).\n","\n"," * **huggingface_hub**: Enables downloading pre-trained models from the Hugging Face Hub.\n","* The --force-reinstall and --upgrade flags ensure you have the latest versions."],"metadata":{"id":"8DLNeAnjpqT8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8e_MxxjMpJ9d"},"outputs":[],"source":["# GPU llama-cpp-python\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n","!pip install huggingface_hub\n","!pip install llama-cpp-python==0.1.78\n","!pip install numpy==1.23.4"]},{"cell_type":"markdown","source":["âœ… ***Define Model Details:***\n","\n","The code specifies the model name `model_name` and the filename `model_basename` of the specific Quantized Llama-2-13B variant optimized for chat interactions. Get `model_name` and `model_basename` from the hugging-face's model card."],"metadata":{"id":"CR7IUzUPqm5R"}},{"cell_type":"code","source":["model_name = \"TheBloke/Llama-2-13B-chat-GGML\"\n","model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\""],"metadata":{"id":"NLg8fh8iqQ4X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… Download Quantinized Model:\n","\n","* The `hf_hub_download` function from `huggingface_hub` is used to download the Quantized model from the Hugging Face Hub repository specified by `model_name`.\n","\n","* The downloaded file is saved to model_path."],"metadata":{"id":"ipihZNniranV"}},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download"],"metadata":{"id":"tbFyaGh8qsue"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from llama_cpp import Llama"],"metadata":{"id":"dMOxZ4z8rjnq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = hf_hub_download(repo_id=model_name, filename=model_basename)"],"metadata":{"id":"tz38tv50rmrb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… ***Initialize Llama LLM (GPU Configuration):***:\n","\n","* A Llama object (lcpp_llm) is created to represent the Llama LLM instance.\n","\n","* Key arguments for initialization include:\n","\n"," * `model_path`: Path to the downloaded model.\n","\n"," * `n_threads`: Number of CPU threads to use (set to 2 in this case).\n","\n"," * `n_batch`: Batch size for processing text sequences (consider GPU VRAM limitations).\n","\n"," * `n_gpu_layers`: Number of GPU layers to leverage for computations. *This value should be adjusted based on your specific model and available GPU VRAM.*"],"metadata":{"id":"pW6XmHVBsGT2"}},{"cell_type":"code","source":["# GPU\n","lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2, # CPU cores\n","    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n","    )"],"metadata":{"id":"pA3-S_olsAD-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… ***Craft Prompt Template:***\n","\n","* A variable `prompt_template` is defined as a formatted string. This template serves two purposes:\n","\n"," * Defines roles: \"SYSTEM\" represents the helpful assistant, and \"USER\" represents the user's prompt.\n","\n"," * Incorporates the actual user prompt (prompt) within the template."],"metadata":{"id":"3xOmXtuCszDj"}},{"cell_type":"code","source":["prompt = \"There exists a unique increasing geometric sequence of five 2-digit positive integers. What is their sum?\"\n","prompt_template=f'''SYSTEM: You are a helpful, mathematical and reasonable assistant. Always answer as accurately.The answer should be given as a non-negative modulo 1000.\n","\n","USER: {prompt}\n","\n","ASSISTANT:\n","'''"],"metadata":{"id":"PEgIZfrJsSOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["âœ… ***Generate Response and Extract Answer:***\n","\n","* The `lcpp_llm` object is called with the `prompt_template` as input.\n","\n","* Additional arguments for generation control:\n","\n"," * `max_tokens`: Maximum number of tokens to generate (225 in this case).\n","\n"," * `temperature`: Controls randomness in the generated text (0.5 for some variation).\n","\n"," * `top_p`: Focuses generation on high-probability tokens (0.95 for more likely outputs).\n","\n"," * `repeat_penalty`: Discourages repetitive tokens (1.2 for diversity).\n","\n"," * `top_k`: Considers only the top k most likely tokens at each step (150 for a balance).\n","\n"," * `echo`: Prints the prompt and generated response together for clarity.\n","\n","* The `response` variable holds the generated text by the LLM.\n","\n","* The code then extracts the user's question and the model's answer from the `response` using response[\"choices\"][0][\"text\"]."],"metadata":{"id":"gE9Qh0OytQ1x"}},{"cell_type":"code","source":["response=lcpp_llm(prompt=prompt_template, max_tokens=225, temperature=0.5, top_p=0.95,\n","                  repeat_penalty=1.2, top_k=150,\n","                  echo=True)"],"metadata":{"id":"O--aIqRQtrgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(response)"],"metadata":{"id":"VUjaYGs2t-Ho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(response[\"choices\"][0][\"text\"])"],"metadata":{"id":"J_TLPpFquFoz"},"execution_count":null,"outputs":[]}]}