{"cells":[{"cell_type":"markdown","metadata":{"id":"_M3vZGnroK6N"},"source":["## ***Unleashing the Power of the Quantized Models from the Hugging Face Community: A Step-by-Step Guide*** ðŸ’»\n","\n","This notebook empowers you to learn how to utilize the Quantized Llama-2-13B model. The code demonstrates how to install necessary libraries, download the Quantized model, configure the Llama LLM for GPU acceleration, and interact with the model to solve a sample problem.\n","\n","### ðŸ”² ***Goal :***\n","\n","*This notebook aims to :*\n","\n","* Install required libraries for managing the Llama LLM and interacting with Hugging Face Hub.\n","\n","* Download the Quantized Llama-2-13B model specifically fine-tuned for chat.\n","\n","* Set up the Llama LLM for GPU processing to maximize performance.\n","Craft a prompt template incorporating user input and assistant role definition.\n","\n","* Generate a response using the model and extract the answer."]},{"cell_type":"markdown","metadata":{"id":"8DLNeAnjpqT8"},"source":["### ðŸ”² ***Steps :***\n","\n","âœ… ***Install and Upgrade Libraries :***\n","\n","* The code starts by installing (or upgrading) necessary libraries:\n","\n"," * **llama-cpp-python**: Provides core functionalities to interact with the Llama LLM.\n","\n"," * **numpy**: Used for numerical computations (potentially involved in the model's processing).\n","\n"," * **huggingface_hub**: Enables downloading pre-trained models from the Hugging Face Hub.\n","* The --force-reinstall and --upgrade flags ensure you have the latest versions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8e_MxxjMpJ9d"},"outputs":[],"source":["# GPU llama-cpp-python\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n","%pip install huggingface_hub\n","%pip install llama-cpp-python==0.1.78\n","%pip install numpy==1.23.4"]},{"cell_type":"markdown","metadata":{"id":"CR7IUzUPqm5R"},"source":["âœ… ***Define Model Details:***\n","\n","The code specifies the model name `model_name` and the filename `model_basename` of the specific Quantized Llama-2-13B variant optimized for chat interactions. Get `model_name` and `model_basename` from the hugging-face's model card."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLg8fh8iqQ4X"},"outputs":[],"source":["model_name = \"TheBloke/Llama-2-13B-chat-GGML\"\n","model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\""]},{"cell_type":"markdown","metadata":{"id":"ipihZNniranV"},"source":["âœ… Download Quantinized Model:\n","\n","* The `hf_hub_download` function from `huggingface_hub` is used to download the Quantized model from the Hugging Face Hub repository specified by `model_name`.\n","\n","* The downloaded file is saved to model_path."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbFyaGh8qsue"},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","from llama_cpp import Llama\n","model_path = hf_hub_download(repo_id=model_name, filename=model_basename)"]},{"cell_type":"markdown","metadata":{"id":"pW6XmHVBsGT2"},"source":["âœ… ***Initialize Llama LLM (GPU Configuration):***:\n","\n","* A Llama object (lcpp_llm) is created to represent the Llama LLM instance.\n","\n","* Key arguments for initialization include:\n","\n"," * `model_path`: Path to the downloaded model.\n","\n"," * `n_threads`: Number of CPU threads to use (set to 2 in this case).\n","\n"," * `n_batch`: Batch size for processing text sequences (consider GPU VRAM limitations).\n","\n"," * `n_gpu_layers`: Number of GPU layers to leverage for computations. *This value should be adjusted based on your specific model and available GPU VRAM.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pA3-S_olsAD-"},"outputs":[],"source":["N_THREADS = 2\n","N_BATCH = 512\n","N_GPU_LAYERS = 32\n","\n","# GPU\n","lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=N_THREADS, # CPU cores\n","    n_batch=N_BATCH, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=N_GPU_LAYERS # Change this value based on your model and your GPU VRAM pool.\n","    )"]},{"cell_type":"markdown","metadata":{"id":"3xOmXtuCszDj"},"source":["âœ… ***Craft Prompt Template:***\n","\n","* A variable `prompt_template` is defined as a formatted string. This template serves two purposes:\n","\n"," * Defines roles: \"SYSTEM\" represents the helpful assistant, and \"USER\" represents the user's prompt.\n","\n"," * Incorporates the actual user prompt (prompt) within the template."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEgIZfrJsSOA"},"outputs":[],"source":["prompt = \"What the trends in AI/ML field ?\"\n","prompt_template=f'''SYSTEM: You are a helpful, mathematical and reasonable assistant. Always answer as accurately.The answer should be given as a non-negative modulo 1000.\n","\n","USER: {prompt}\n","\n","ASSISTANT:\n","'''"]},{"cell_type":"markdown","metadata":{"id":"gE9Qh0OytQ1x"},"source":["âœ… ***Generate Response and Extract Answer:***\n","\n","* The `lcpp_llm` object is called with the `prompt_template` as input.\n","\n","* Additional arguments for generation control:\n","\n"," * `max_tokens`: Maximum number of tokens to generate (225 in this case).\n","\n"," * `temperature`: Controls randomness in the generated text (0.5 for some variation).\n","\n"," * `top_p`: Focuses generation on high-probability tokens (0.95 for more likely outputs).\n","\n"," * `repeat_penalty`: Discourages repetitive tokens (1.2 for diversity).\n","\n"," * `top_k`: Considers only the top k most likely tokens at each step (150 for a balance).\n","\n"," * `echo`: Prints the prompt and generated response together for clarity.\n","\n","* The `response` variable holds the generated text by the LLM.\n","\n","* The code then extracts the user's question and the model's answer from the `response` using response[\"choices\"][0][\"text\"]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O--aIqRQtrgM"},"outputs":[],"source":["params = {\n","    'prompt': prompt_template,\n","    'max_tokens': 225,\n","    'temperature': 0.5,\n","    'top_p': 0.95,\n","    'repeat_penalty': 1.2,\n","    'top_k': 150,\n","    'echo': True\n","}\n","response = lcpp_llm(**params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VUjaYGs2t-Ho"},"outputs":[],"source":["print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_TLPpFquFoz"},"outputs":[],"source":["print(response[\"choices\"][0][\"text\"])"]},{"cell_type":"markdown","metadata":{},"source":["### ***THANKS*** and good luck !!\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPn5LAiSbttPjJ6WkPZrnnW","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
